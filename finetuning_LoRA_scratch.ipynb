{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0a6c37-b3cc-4cb9-b5f1-5825e75fd788",
   "metadata": {},
   "source": [
    "# Finetuning with LoRA\n",
    "In this notebook, I am implementing the LoRA finetuning procedure from scratch.\n",
    "\n",
    "## Introduction to LoRA\n",
    "The rationale behind LoRA is to allow finetuning a pretrained LLM with reduced GPU RAM requirement and potentially computational power. Most of the parameters reside in the dense projection matrices $W_Q, W_K, W_V$ that project (linearly) the input to the transformer layer to the keys (for query and key) $v_k$ and values space $v_v$. Usually, all these spaces (including the input) are equal to the embedding space, which can be large. This means that both the input and output spaces of these matrices $W$ are $d_{emb}^2$, which can get large fast (like for embeddings of 1k, each such matrix has 1 million parameters) and are the bulk of the parameters of the transformer layer and the LLM network at large.\n",
    "\n",
    "The idea behind LoRA is to substitute these with an alternative with a much smaller parameter space. We can do this by fine-tuning sparse matrices and only using the dense $W$ matrices for inference. More conretely, we can have matrices $A \\in \\mathbb{R}^{d_{emb} \\times k}$ and $B \\in \\mathbb{R}^{k \\times d_{emb} }$ with $k \\ll d_{emb}$ and fine-tune $AB \\in \\mathbb{R}_{d_{emb} \\times d_{emb}}$ on the new dataset.\n",
    "\n",
    "Now, for one projection of the transformer (let's say $Q$, we have the following:\n",
    "$$ Q = W_Q + A_Q \\cdot B_Q $$\n",
    "where $W_Q$ is pretrained and frozen and we optimize $A_Q$ and $B_Q$\n",
    "\n",
    "And, by choosing a small enough $k$, we can reduce the total RAM needed by a huge margix. The total trainable parameters for LoRA are $d_{emb} \\cdot k + k \\cdot d_{emb} = 2k d_{emb} \\ll d_{emb}^2$. Essentially, if we treat $k$ as a constant, the number of trainable parameters becomes linear $O(d_{emb})$.\n",
    "\n",
    "If we have $n_{l}$ number of transformer layer (in GPT-2, usually $n_l = 12$), each having 3 projections (for $Q$, $K$, $V$), now the number of trainable parameters for the dense part of the transformer becomes $3n_l \\cdot 2k d_{emb} = 6n_l k d_{emb}$. There has been research to do something similar with the MLP component of the transformer (ie have $A$ and $B$ matrices for it), but like in its basic variant, in LoRA we similarly freeze its parameters like in the dense matrices.\n",
    "\n",
    "### Action Plan\n",
    "In my implementaion, I am retaining the GPT-2 architecture developed in ch04 and I am enhancing it with the LoRA mechanism. Additionally, I load a pretrained GPT-2 model from hugging face and transfer its parameters to the book implementation of GPT-2. Then, I freeze all these parameters, allowing only the ones introduced as part of the LoRA module to be trained.\n",
    "\n",
    "## Implementation\n",
    "### Pretrained GPT-2 model\n",
    "Next, I am downloading a pretrained GPT-2 model from Huggingface, implemented in PyTorch. I am doing some analysis to figure out its inner workings. Later on, I will be endowing it with the LoRA mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e889b8-80de-4728-a3bb-c67b7a908a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e3ac09-e9e9-472b-9117-7dc8f44dc545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3a1c89cb-0e87-403d-965e-6d0073d68a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"openai-community/gpt2\",\n",
    "    device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8481a01b-33e5-40db-b39c-6d2d9504efe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(isinstance(pretrained_model, torch.nn.Module)) # our pretrained model is PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d224576a-9b12-409b-88e6-c593d0ee1895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1600)\n",
      "    (wpe): Embedding(1024, 1600)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-47): 48 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=4800, nx=1600)\n",
      "          (c_proj): Conv1D(nf=1600, nx=1600)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=6400, nx=1600)\n",
      "          (c_proj): Conv1D(nf=1600, nx=6400)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580049d7-a231-4d89-948a-91fc52ac2af1",
   "metadata": {},
   "source": [
    "As we can see from the above PyTorch network overview for the Huggingface GPT2 model, it actually implements the GPT-2 XL model specifications (48 layers, 25 heads, 1600 embedding/model dimension). This means we have $1600 \\div 25 = 64$ dimensions per head.\n",
    "\n",
    "The thing we would need to modify is the `c_proj` layer, which is a fancy way (`Conv1D`) to do the dense matrix multiplication. What `c_proj` does is it maps the input of the transformer to a space 3 times the input, which corresponds to the concatenated vector that consists of the three input vectors ($Q$, $K$, $V$ each having dimension 1600). One can find more details about the inner workings of the Huggingface code in the following link: https://huggingface.co/transformers/v3.5.1/_modules/transformers/modeling_gpt2.html\n",
    "\n",
    "The crux of the attention mechanism is implemented in `class Attention` in the code (one can do Ctrl+F to search for this term and find it in the code). For clarity, in the cell below, I am listing the contents of this Attention class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd9cc09a-d9c4-44b8-a51c-b74edb351319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Attention(\n",
      "  (c_attn): Conv1D(nf=4800, nx=1600)\n",
      "  (c_proj): Conv1D(nf=1600, nx=1600)\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_model.transformer.h[0].attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6695454-19fd-42be-ba13-a4ccb89b5b93",
   "metadata": {},
   "source": [
    "Essentially, we are interested in the following line of code:\n",
    "``` \n",
    "query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
    "```\n",
    "\n",
    "We would need to endow `c_attn` with the LoRA mehanism. That is, we retain the parameters of `c_attn` and add to it the output of the LoRA mechanism (ie. the matrix multiplication $AB$ for $Q$, $K$, and $V$). One key thing is to properly initialize the $A$ and $B$ matrices. $A$ should have values drawn from normal distribution with mean 0 and std 1 (ie $A \\sim \\mathcal{N}(0, 1)$) and $B$ should be all zeros.\n",
    "\n",
    "### GPT-2 with LoRA\n",
    "I am implementing the LoRA mechanism below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c700b8e-a9e3-4b15-842f-d87cba17f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation for GPT2 Module on \n",
    "# https://huggingface.co/transformers/v3.5.1/_modules/transformers/modeling_gpt2.html\n",
    "# query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e9d84916-8098-4356-965d-5405f87df783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, embed_dim, k, c_attn):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_attn = c_attn\n",
    "        \n",
    "        self.A_q = nn.Linear(embed_dim, k)\n",
    "        self._init_A(self.A_q)\n",
    "        \n",
    "        self.A_k = nn.Linear(embed_dim, k)\n",
    "        self._init_A(self.A_k)\n",
    "        \n",
    "        self.A_v = nn.Linear(embed_dim, k)\n",
    "        self._init_A(self.A_v)\n",
    "\n",
    "        self.B_q = nn.Linear(k, embed_dim)\n",
    "        self._init_B(self.B_q)\n",
    "        \n",
    "        self.B_k = nn.Linear(k, embed_dim)\n",
    "        self._init_B(self.B_k)\n",
    "        \n",
    "        self.B_v = nn.Linear(k, embed_dim)\n",
    "        self._init_B(self.B_v)\n",
    "\n",
    "    def _init_A(self, A):\n",
    "        A.weight = torch.nn.Parameter( torch.normal(mean=0, std=1, size=A.weight.size()) ) # N(0, 1)\n",
    "\n",
    "    def _init_B(self, B):\n",
    "        B.weight = torch.nn.Parameter( torch.zeros(B.weight.size()) ) # all zeros\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q_sparse = self.B_q(self.A_q(x))\n",
    "        K_sparse = self.B_k(self.A_k(x))\n",
    "        V_sparse = self.B_v(self.A_v(x))\n",
    "\n",
    "        lora_out = torch.concat([ Q_sparse, K_sparse, V_sparse ], dim=-1) # (B, embed_dim, 3 * embed_dim)\n",
    "        original_out = self.c_attn(x)\n",
    "\n",
    "        return original_out + lora_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "70b62063-0a6e-42ac-af5e-bb3f636d5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora = LoRA(1600, 10, nn.Linear(1600, 4800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e3c0daf-bc91-4df6-a6be-7f6a960cdc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA(\n",
      "  (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
      "  (A_q): Linear(in_features=1600, out_features=10, bias=True)\n",
      "  (A_k): Linear(in_features=1600, out_features=10, bias=True)\n",
      "  (A_v): Linear(in_features=1600, out_features=10, bias=True)\n",
      "  (B_q): Linear(in_features=10, out_features=1600, bias=True)\n",
      "  (B_k): Linear(in_features=10, out_features=1600, bias=True)\n",
      "  (B_v): Linear(in_features=10, out_features=1600, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.3343,  0.8559, -0.0994,  ...,  2.0686, -0.7781,  1.3928],\n",
      "        [ 0.8182, -0.5697,  1.3911,  ..., -0.6387, -2.0151, -0.3035],\n",
      "        [ 0.1300, -0.1492, -0.5913,  ...,  0.8438,  0.0059, -1.9801],\n",
      "        ...,\n",
      "        [ 1.1793,  0.3672,  0.0205,  ...,  0.5051,  0.3859, -0.3209],\n",
      "        [ 0.6594, -0.9072,  0.5443,  ...,  0.2693,  0.0189,  0.1251],\n",
      "        [ 0.5874, -0.3191, -1.1967,  ..., -1.1124, -0.3248,  1.0687]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(lora)\n",
    "print(lora.A_q.weight) # values should be mostly on [-1, 1]\n",
    "print(lora.B_q.weight) # all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d12069c8-3b59-40bb-adf5-ca7ed13a5663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4800])\n"
     ]
    }
   ],
   "source": [
    "print(lora(torch.zeros((32, 1600))).size()) # it works, as it maps from 1600 to 3 * 1600 = 4800, with 32 being the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f505af8e-72fe-4b81-bcfe-d66a00a7911e",
   "metadata": {},
   "source": [
    "Okay, our LoRA module seems to work pretty well. Now, I am adding it to the GPT-2 model. Essentially, we need to iterate through `h` and, for each of the 48 layers, replace `c_attn` with a fresh `LoRA` initialization. Also, we should freeze all the parameters before doing this modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f66e413-2439-478f-b6de-ff7c5736b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a55846e2-760e-42a4-b45a-c0c0b9f41d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"openai-community/gpt2\",\n",
    "    device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dd8eba9e-e8c4-4d18-ac45-f3799cc62414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's freeze the model\n",
    "\n",
    "def freeze(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "freeze(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a8638c6-5242-4fe1-9cea-1c2f22f62e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, I am adding the LoRA component, note that this part is not frozen.\n",
    "for transformer_layer in lora_model.transformer.h:\n",
    "    lora_instance = LoRA(768, 10, transformer_layer.attn.c_attn).to(device) # k=10\n",
    "\n",
    "    transformer_layer.attn.c_attn = lora_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "378f9949-13d4-46ac-8436-b674e81a0f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): LoRA(\n",
      "            (c_attn): Conv1D(nf=2304, nx=768)\n",
      "            (A_q): Linear(in_features=768, out_features=10, bias=True)\n",
      "            (A_k): Linear(in_features=768, out_features=10, bias=True)\n",
      "            (A_v): Linear(in_features=768, out_features=10, bias=True)\n",
      "            (B_q): Linear(in_features=10, out_features=768, bias=True)\n",
      "            (B_k): Linear(in_features=10, out_features=768, bias=True)\n",
      "            (B_v): Linear(in_features=10, out_features=768, bias=True)\n",
      "          )\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lora_model) # SUCCESS, note how c_attn got our LoRA module, for all 0 to 47 layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1bcac3-6ed2-471c-b71d-b2a2fbf32b1f",
   "metadata": {},
   "source": [
    "## Finetuning Experiment\n",
    "### Tokenizer load\n",
    "Next, I am training `lora_model` on the sarcastic dataset, using $k=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddf7ba9f-f7c3-4467-98b4-cc1cb99b96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for GPT-2\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'openai-community/gpt2',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "015d6caf-0fd5-4760-a38b-82980a744e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 428, 318, 8047, 0]\n",
      "Hello\n",
      "<|endoftext|>\n",
      "50256\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('Hello, this is Nick!'))\n",
    "print(tokenizer.decode([ 15496 ]))\n",
    "\n",
    "print(tokenizer.eos_token) # this is our de facto padding token to be used for dataset preprocessing\n",
    "\n",
    "padding_idx = tokenizer.encode(tokenizer.eos_token)[0]\n",
    "print(padding_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25016e39-c0d9-4a4c-8228-36f80ccd52b4",
   "metadata": {},
   "source": [
    "### Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d262eae2-8c2f-488e-b514-081ee2fff0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sarcasm_finetune.txt', 'r') as f:\n",
    "    dset = f.read()\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for line in dset.split('\\n'):\n",
    "    if line != '':\n",
    "        prompts.append(line)\n",
    "# Rule \", ', -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de69659b-14ed-4352-9cbe-ab48c856ffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69641b02-6146-4429-b1d7-41d716624f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SarcasticDataset(Dataset):\n",
    "    def __init__(self, text_prompts, tokenizer, pad_idx=50256):\n",
    "        self.text_prompts = text_prompts\n",
    "        self.tokenized_prompts = [ tokenizer.encode(prompt) for prompt in text_prompts ]\n",
    "\n",
    "        self._make_equal_length(self.tokenized_prompts, pad_idx)        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.tokenized_prompts[idx], dtype=torch.long)\n",
    "\n",
    "    def _make_equal_length(self, tokenized_prompts, pad_idx):\n",
    "        max_len = len(tokenized_prompts[0])\n",
    "\n",
    "        for prompt in tokenized_prompts:\n",
    "            max_len = max(len(prompt), max_len)\n",
    "\n",
    "        for prompt in tokenized_prompts:\n",
    "            while len(prompt) < max_len:\n",
    "                prompt.append(pad_idx)\n",
    "\n",
    "dset = SarcasticDataset(prompts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24bbd27a-cd90-48b7-b909-6b92c92c5cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5812,  1049,    11,  1194,  3321,    12,  3137,   644,   314,  2622,\n",
      "          284,  2987,   616, 10038,    13, 50256, 50256, 50256, 50256, 50256])\n",
      "[5812, 1049, 11, 1194, 3321, 12, 3137, 644, 314, 2622, 284, 2987, 616, 10038, 13]\n"
     ]
    }
   ],
   "source": [
    "print(dset[0])\n",
    "print(tokenizer.encode(prompts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "490d2663-39d9-4e52-a43d-e3a98ca976f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20])\n",
      "torch.Size([8, 20])\n",
      "torch.Size([8, 20])\n",
      "torch.Size([8, 20])\n",
      "torch.Size([8, 20])\n",
      "torch.Size([8, 20])\n",
      "torch.Size([2, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset=dset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "for text_batch in loader:\n",
    "    print(text_batch.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92d8cd-8795-4bfa-b368-961680297f96",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a37e9bcc-e401-4fa2-b486-14fefb185e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device, tokenizer, num_epochs=5):\n",
    "    #model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader: # (B, T) B: 8, T: 20\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            inputs = batch[:, :-1] # (B, T-1)\n",
    "            targets = batch[:, 1:] # (B, T-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs).logits # (B * (T - 1), vocab_size) # [ 0.1, 0.3, 0, 0, ..., 0.6] 10\n",
    "\n",
    "            outputs = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "            targets = targets.contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92252e41-022c-4e74-81c8-81db48be4df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(lora_model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=50256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "83dd48fe-fdd6-40c4-be2d-31bb619266a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(lora_model, loader, optimizer, criterion, device, tokenizer, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a3f39-cdda-427b-a325-0ae3ef0eeb60",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d3413397-bac3-43e5-acd2-6cba9a359319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied code from ch04\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:] # 1000, 50 [930 - 980]: 981, [931 - 981]: 982\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond).logits\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :] # 982\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def generate_text(model, start_context, tokenizer):\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    \n",
    "    encoded_tensor = torch.tensor(encoded, device=device).unsqueeze(0)\n",
    "    \n",
    "    model.eval() # disable dropout\n",
    "    \n",
    "    out = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=encoded_tensor,\n",
    "        max_new_tokens=100,\n",
    "        context_size=1024\n",
    "    )\n",
    "    \n",
    "    numpy_out = out.cpu().numpy()[0]\n",
    "\n",
    "    return tokenizer.decode(numpy_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc049cdd-06ab-46f8-beb1-72701e6d0ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"when I wake up from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night' from a bad night'\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lora_model, 'when I wake up', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ddbfbaca-48e8-41fd-a193-02e868a616ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"when I wake up, I'm going to be in a coma for a while. I'm going to be in a coma for a while. I'm going to be in a coma for a while. I'm going to be in a coma for a while. I'm going to be in a coma for a while. I'm going to be in a coma for a while. I'm going to be in a coma for a while. I'm going to be in a coma for a while. I'm going\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(pretrained_model, 'when I wake up', tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
